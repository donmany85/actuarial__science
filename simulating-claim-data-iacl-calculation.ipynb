{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":">**SIMULATING CLAIMS DATA & INFLATION ADJUSTED CHAIN LADDER (IACL) Calculations** \n\n**SUMMARY**\n\nThe objective of this Kernel is to accentuate the application of Python for inflation adjusted chain ladder (IACL) calculations. Specifically, I have focued on just the claim triangles for 'Claim Amounts'. Do comment if you'd like to see 'Claim Numbers' or \"Average cost per claim\" as well. This kernel ends until we arrive at a reserve calculation. I have outlined the coding by chapters below.\n\nI have also only used the Pandas module here as well. Note that the IACL branches from the basic chain ladder. The only difference is the past and future inflation accountability calculations. \n\n**Some pointers to note**\n\n-I have included both the IACL (denoted with \"Inflated\" Headers) and basic chain ladder (denoted with \"Non Inflated\" Headers) codings required side by side for comparative purposes. \n\n-This is mainly self-taught, so do help is you feel any improvements I could use!\n\n-Chapters 1 & 2 ==> improvising random probabilistic claims data\n\n-Chapters 3, 5, 7, 9 ==> Plotted Charts and Table previews of our data\n\n-Chapters 4, 6, 8 & 10 ==> Data manipulations and calculations","metadata":{"_uuid":"3a0137cbc5cfa1a3431a89930e00c43a061b8522"}},{"cell_type":"markdown","source":"**Chapters**\n\n* 1\tIntroduction\n\n        1.1\tImport main modules\n        1.2\tEstablish random data range (Policy Counts, Years, Dates)\n        1.3\tOther Assumptions\n    \n* 2\tCreate Ranomized Claims Data\t\n\n        2.1\tInsured IDs\n        2.2\tInsured Date\n        2.3\tClaim Numbers\n        2.4\tClaim Amounts   \n        2.5\tTransaction Date\n    \n* 3\tTable Preview Raw Data (Part A)\n\n        3.1\tRanomized Claims Data\n    \n* 4\tQuick DataCleaning\n\n        4.1\tExtract Years or Quarters (Depending on your choice of lag period)\n    \n5\tCalculations (Part A)\n\n        5.1\tYear Lags\n        5.2\tCompile Past Claims Data (Incremental & Cumulative Amounts)\n        5.3\tEstablish Inflation Indexes\n        5.4\tUplift (Past Inflation) Incremental Amounts \n        5.5\tDerive corresponding uplifted Cumulative Amounts\n        5.6\tIndividual Loss Development Factors\n    \n* 6\tPlot & Table Preview Triangles  (Part B)\t\n\n        6.0\tDefine General Plot Functions\n        6.1\tIncremental Amount\n        6.2\tCumulative Amounts\n        6.3\tIndividual Loss Development Factors\n    \n* 7\tCalculations (Part B)\n\n        7.1\tEstablish Predicted_df\n        7.2\tCoordinates (InsuredYear by LagYr) of predicted cells\n        7.3\tImpute latest Cumulative Amounts available (as a base point for multiplying by LDF)\n        7.4\tSimpleMeanLoss & Volume Weighted & Last 5 and 3 year & Selected LDF\n        7.5\tPredicted Cumulative Amounts = Uplift previous Cumulative Amounts by LDF\n        7.6\tData-type adjustments (int & float)\n        7.7\tPredicted Incremental Amount\n        7.8\tProject (Future Inflation) Predicted Incremental Amount\n    \n* 8\tPlot & Table Preview Predictions  (Part C)\n\n        8.1\tIncremental Amount\n        8.2\tCumulative Amounts\n\n* 9\tPlot Full Cumulative Triangle (Part D)\n\n        9.0\tDefine General Plot Functions\n        9.1\tNon Inflated Claims\n        9.2\tInflated Claims\n\n* 10 Reserves\n\n        10.1 Inflated Amounts\n        10.2 Non Inflated Amounts","metadata":{"_uuid":"b089c8f2ce7265f175c9a8272660b02d8643893f"}},{"cell_type":"markdown","source":"* **1. Introduction**","metadata":{"_uuid":"c12458b2f7f18cdb0f9ed0a83200e15d57a6f872"}},{"cell_type":"markdown","source":"**1.1** Import main modules\n\nWe will first import the main modules","metadata":{"_uuid":"58ec9751dec71ea2ba432d2097868f6c27858b06"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\npd.options.display.max_columns = 100","metadata":{"_uuid":"ba5d6cd8a20aef316d4ae20d55a2123df376262f","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"**1.2**\tEstablish random data range (Policy Counts, Years, Dates)\n\nNow to set the breadth of data we are going to work with","metadata":{"_uuid":"1ac35b1f8c9d795fe6b5d0c7bbc76f49bacf899d"}},{"cell_type":"code","source":"\"\"\"Define parameters of data\"\"\"\n# Number of entries\nPolicyCount = 15000\n# 9-year period\nYearEndCap = 2017\nYearStartCap = 2007\n# Dates\nDateEndCap = datetime.date(YearEndCap, 12, 31)    # year, month, day\nDateStartCap = datetime.date(YearStartCap, 12, 31)  # year, month, day","metadata":{"_uuid":"73839e69e485836bd23241b8ec25001afbc72160","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"**1.3** Other Assumptions","metadata":{"_uuid":"5be7faaf48ccf74f6de7c58d28a5810b8b8ce531"}},{"cell_type":"markdown","source":"-No admin costs, claims paid in bulk on transaction date rather than multiple dates for those with multiple claims, ignoring earned gross premium calculations etc","metadata":{"_uuid":"36cdd567180b86438755b6aafd6ac9fa02883e06"}},{"cell_type":"markdown","source":"* **2. Create Randomized Claims Data**\n\nNow we will use randomisation with probabilistic distributions to simulate some claims data. First we establish the initial data-frame.","metadata":{"_uuid":"50aaf9b0a36fff5038551104b849815358f0015b"}},{"cell_type":"code","source":"\"\"\"Create Main DataFrame filled with NaN's\"\"\"\n# Establish initial data-frame\ncolumns_1 = ['Insured_ID', 'Insured_Date', 'Claims_Number', 'Claims_Amount', 'Transaction_Date',\n           'Insured_Year', 'Insured_Quarter',\n           'Transaction_Year', 'Transaction_Quarter']\nClaimsData = pd.DataFrame(columns=columns_1)","metadata":{"_uuid":"ef9a0bfd4d6e51b7e2980d6f877686794b4d99f9","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"**2.1** Insured IDs\n\nThis is just for labelling purposes to simulate some reality to this. Do note that I have hidden some outputs to neaten up this Kernel. *Unhide to view output","metadata":{"_uuid":"ebd2156a7bfe779ac6ebd7eff8f5dc3e41b1a10d"}},{"cell_type":"code","source":"# Insured_ID's\nClaimsData['Insured_ID'] = list(range(1, PolicyCount+1))\nprint(ClaimsData['Insured_ID'])","metadata":{"_uuid":"cb827155dc7b33fcd67a1109bee05d390a777198","_kg_hide-output":true,"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"**2.2** Insured Dates\n\nThis is the date where the insured signs for their policy. Here we will use a random selection of dates between our date range. *Unhide to view output","metadata":{"_uuid":"09ed87262b06837bdf00f2ed4814d98f75d964b0"}},{"cell_type":"code","source":"# Insured_Date's\n# Random distribution\nimport random\nfor row in range(0, PolicyCount):\n    n_days = (DateEndCap-DateStartCap).days\n    random_days = random.randint(0, n_days-1)\n    Random_Insured_Date = DateStartCap + datetime.timedelta(days=1) + datetime.timedelta(days=random_days)\n    ClaimsData.loc[row, 'Insured_Date'] = Random_Insured_Date\nprint(ClaimsData['Insured_Date'])","metadata":{"_uuid":"fd30651260f04724bff7cbb8f39efe96c57d4c85","_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"**2.3** Claims Numbers\n\nWe will now simulate the claim numbers using randomised poisson distribution. In real world scenarios, this is the most common distribution for claim numbers. *Unhide to view output","metadata":{"_uuid":"7cfb9f6257aebee46f2acccea98f403991d76681"}},{"cell_type":"code","source":"# Claims_Number's\n# Poisson random distribution\n# Poisson parameters\nLambda = 10\nSize = 1\nfor row in range(0, PolicyCount):\n    ClaimCount = np.random.poisson(1, 1)\n    ClaimsData.loc[row, 'Claims_Number'] = ClaimCount\n\n# Remove the square brackets (i.e.a list within a list) by passing into a list & back into df again\nClaimsData['Claims_Number'] = pd.DataFrame(ClaimsData['Claims_Number'].values.tolist())\nprint(ClaimsData['Claims_Number'])","metadata":{"_uuid":"eb2e0ba2453887ea3253dbf8542a2272598c828e","_kg_hide-output":true,"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"**2.4** Claim Amounts\n\nSimilarly, for claim amounts we will also use randomisation but with Log Normal distribution instead. Do note that here we use a nested loop referncing code. Where if the random claim number was 0 we will have 0 claim amount. For cases without 0 random claim numbers, we will respectively generate and sum 'n' number of random Log Normal distributed claim amount. E.g. Claim Number 1 will have 1 random Log Normal distributed amount, Claim Number 2 will have the sum of 2 random Log Normal distributed amount etc.\n\nI have also included a spare repetitive Min Max function to simulate reinsurance if need. The code is a 'Do While' loop so as to not disrupt the distribution of claim amounts. But have excluded it in the case. \n\n*Unhide to view output","metadata":{"_uuid":"8781fc8764d7d596aa50acb58f952d67a578a5d9"}},{"cell_type":"code","source":"\"\"\"Special Case if need to simulate claims amount minimum & maximum limit. E.g. Reinsurance cases XOL\"\"\"\nimport random\ndef trunc_amt(mu, sigma, bottom, top):\n    a = random.lognormal(mu,sigma)\n    while (bottom <= a <= top) == False:\n        a = random.lognormal(mu,sigma)\n    return a","metadata":{"_uuid":"9d6ee560a400b252b599ded17d49d6f16efe3af5","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Claims_Amount's\n# Gaussian random distribution\n# Gaussian parameters\nMeanClaimAmt = 10\nStdDevClaimAmt = 4\nfor row in range(0, PolicyCount):\n    if ClaimsData.loc[row, 'Claims_Number'] == 0:\n        # Impute 0 so that ClaimAmount is 0\n        ClaimsData.loc[row, 'Claims_Amount'] = 0\n    else:\n        ClaimNumber = ClaimsData.loc[row, 'Claims_Number']\n        num = np.random.lognormal(MeanClaimAmt, StdDevClaimAmt, ClaimNumber).sum()\n        ClaimsData.loc[row, 'Claims_Amount'] = num\n\n# Remove the square brackets (i.e.a list within a list) by passing into a list & back into df again\nClaimsData['Claims_Amount'] = pd.DataFrame(ClaimsData['Claims_Amount'].values.tolist())\nprint(ClaimsData['Claims_Amount'])","metadata":{"_uuid":"347a633d5db24a1b2abbf68d6280e42f2a38c51c","_kg_hide-output":true,"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"**2.5** Transaction Dates\n\nThe transaction dates are the dates that the insurer paid to the insured for a claim made. Just as in claim amounts, we will do a nested loop referencing. Where if the claim number was 0 we will input Transaction Date as the Insured Date, to achieve a 0 lag year. While for cases without 0 as the claim numbers, we will generate a random date between the 'Insured Date' and the YearEndCap of out data range. *Unhide to view output","metadata":{"_uuid":"2c39e66539fbde33da6b88737b49d4b8de2996c5"}},{"cell_type":"code","source":"# Transaction_Date's\n# Random distribution\nimport random\nfor row in range(0, PolicyCount):\n    DateStart = ClaimsData.loc[row, 'Insured_Date']\n    if ClaimsData.loc[row, 'Claims_Number'] == 0:\n        # Impute InsuredDate so that Lag(i.e.DevelopmentPeriod) will be 0\n        ClaimsData.loc[row, 'Transaction_Date'] = DateStart\n    elif (DateEndCap-DateStart).days <=0:\n        ClaimsData.loc[row, 'Transaction_Date'] = DateStart\n    else:\n        n_days = (DateEndCap-DateStart).days\n        random_days = random.randint(1, n_days) # Min 1 day to avoid conflict of zero days and no claims\n        Random_Transaction_Date = DateStart + datetime.timedelta(days=random_days)\n        ClaimsData.loc[row, 'Transaction_Date'] = Random_Transaction_Date\nprint(ClaimsData['Transaction_Date'])","metadata":{"_uuid":"f8adc08d5fbc1b632c333fca999c14eea876b0ca","_kg_hide-output":true,"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"* **3. Preview Raw Data (Part A)**","metadata":{"_uuid":"172f6acb5f5faf8fe0f99717be94807d31649b73"}},{"cell_type":"markdown","source":"**3.1** Randomized Claims Data\n\nNow to preview our random data that we have improvised so far.","metadata":{"_uuid":"fef531f6e1db76bc2a8e3c3b80dffa5dd561b4bb"}},{"cell_type":"code","source":"display(ClaimsData.head(10))","metadata":{"_uuid":"19c4992237733a9a617cd6f5c5f492ea05f400f2","trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"* **4. Preview Raw Data (Part A)**","metadata":{"_uuid":"9e64acbe44dc25b3f4fd6b62ce1fb6aa98bd6099"}},{"cell_type":"markdown","source":"**4.1** Extract Years or Quarters (Depending on your choice of lag period)\n\nNow to prepare our calculation for the different development periods. I have chosen years as the development period here. You may use months alternatively. *Unhide to view output\n\n*Do note that I will use lag year synonymously with development year here.","metadata":{"_uuid":"0660bdff6623493bbaaf1b3f1ffe19de0e71b82c"}},{"cell_type":"code","source":"# Extract & Impute Date Components\n# Jan-Mar=1, Apr-Jun=2, July-Sep=3, Oct-Dec=4\n# Insured Year\nClaimsData['Insured_Year'] = ClaimsData['Insured_Date'].apply(lambda x: x.year)\nClaimsData['Transaction_Year'] = ClaimsData['Transaction_Date'].apply(lambda x: x.year)\n# Insured Month\nClaimsData['Insured_Quarter'] = ClaimsData['Insured_Date'].apply(lambda x: x.month)\nClaimsData['Transaction_Quarter'] = ClaimsData['Transaction_Date'].apply(lambda x: x.month)\nprint(ClaimsData[['Insured_Date', 'Insured_Year', 'Transaction_Date', 'Transaction_Year']])","metadata":{"_uuid":"32cc2ae74dce6aec37680ad4435fd54cabd1c5b5","_kg_hide-output":true,"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"* **5. Calculations (Part A)**","metadata":{"_uuid":"79d8d44db3305e418322c4a4f33f5b6b83f65ff6"}},{"cell_type":"markdown","source":"**5.1**\tYear Lags\n\nThis is pretty straightforward where we simply take the difference between the \"Transaction year\" and the \"Insured year\" columns. In simple terms, the number of years between the insured seured the policy and when a claim was received. *Unhide to view output","metadata":{"_uuid":"1cc7441c963c5e76a6f45991356e885da56882a7"}},{"cell_type":"code","source":"# Year ONLY lag\nClaimsData['Year_Only_Lag'] = ClaimsData['Transaction_Year'] - ClaimsData['Insured_Year']\nprint(ClaimsData)","metadata":{"_uuid":"3671578172c17068b6759428ed836c0ca699899e","_kg_hide-output":true,"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"**5.2**\tCompile Past Claims Data (Incremental  & Cumulative Amounts)\n\nNow to simply compile the claims data in a sorted format. Specifically, in a default ascending insured year and lag year order. This does help greatly in terms of indexing later on. *Unhide to view output\n\nCode Explanation-\n\nIncremental - We are using the \"Claim Amounts\" as the output column. We then set filtering rows (\"Insured year\") and columns (\"Lag year\"), as we want to see the aggregate for each combination of \"Insured year\" and \"Lag year\". The \"sum\" function acts as a the aggregate function. Subsequently, the \"reset_index\" function is used to force the data-frame to use the default numerical indexing for the leftmost column. We then assign this to a new data-frame \"py_data\"\n\nCumulative - Just as before, we group the data-frame by the \"Insured year\" and output the corresponding \"Claim Amounts\". However, now we instead use the new data-frame \"py_data\" and we do not require the added columns \"Lag year\". In addition, we also now use the \"cumsum\" as a the aggregate function to derive the cumulative claim amounts for each.","metadata":{"_uuid":"57cb8066f3dc7a9ba319cf4e499c4464947b5347"}},{"cell_type":"code","source":"# Compile Past Claims Data\n# Incremental Claims Amount\npy_data = ClaimsData['Claims_Amount'].groupby([ClaimsData['Insured_Year'], ClaimsData['Year_Only_Lag']]).sum().reset_index()\n# Convert into data-frame\npy_data = pd.DataFrame(py_data)\n# Cumulative Claims Amount\npy_data[\"cumsum\"] = py_data[\"Claims_Amount\"].groupby(py_data[\"Insured_Year\"]).cumsum()\nprint(py_data)","metadata":{"_uuid":"2d8de1a365e5dff44addac6670ddf33bd44f6df8","_kg_hide-output":true,"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"**Do note from this point onwards, we are dealing with Inflated Adjusted Chain Ladder (IACL) calculations. As mentioned before, I have separated the codings which are required for the IACL (with \"Inflated\" Headers) from those required for a basic chain ladder (with \"Non Inflated\" Headers)**","metadata":{"_uuid":"ae7f26f12b47ceb080f41daf288b91214818e57e"}},{"cell_type":"markdown","source":"**5.3**\tEstablish Inflation Indexes\n\nFor further real world simulation, we will now use the approximated past UK Inflation rates.","metadata":{"_uuid":"78fb1369e3cb0090c7e8b7c870e006fb835a56c2"}},{"cell_type":"code","source":"# Establish Inflation Index\n# Create data-frame of Cumulative inflation rates\ncolumns_2 = ['Year', 'CumPastInflation']\nInflation_df = pd.DataFrame(columns=columns_2)\n# Past Inflation Years\nInflation_df['Year'] = [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n# Past Inflation Index\nInflation_df['CumPastInflation'] = [1.32, 1.27, 1.28, 1.22, 1.16, 1.12, 1.09, 1.07, 1.05, 1.04, 1.00]\ndisplay(Inflation_df)","metadata":{"_uuid":"1feb0cf461120e9a4d62bec7f51b82d8462a18b2","trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"**5.4**\tUplift (Past Inflation) Incremental Amounts\n\nHere we will account for past inflation for the incremental amounts, NOT the cumulative amounts.  *Unhide to view output\n\nCode Explanation-\n\nHere for each incremental claim amount, we continually iterate through the \"inflation_df\" to derive the corresponding inflation year and indexes and subsequently uplift the amount.\n\nJust as before we will first set the inflated incremental amounts equal to the non-inflated for easy referencing.\n\nThe code executes 2 nested loops. The first loop is for each incremental claim amount in the \"py_data\" data-frame, while the second loop is for the each inflation year and inflation index in the \"inflation_df\" data-frame.\n\nIn the first loop, for each incremental claim amount in the \"py_data\" data-frame, we establish the \"Insured Year\", \"Lag Year\" and \"Transaction year\" (or \"Insured Year\" plus \"Lag Year\").\n\nWith this ongoing first loop, we then have a second loop to iterate through the \"inflation_df\" data-frame and establish the \"inflation year\". While iterating the \"inflation_df\", we set a conditional that upon reaching a equilibrium point where the respective claim amounts year of valuation (or \"Transaction Year\") is equal to the inflation year we will execute the proceeding uplift calculation below.  \n\nWe will now determine the corresponding \"Transaction year\" and year-end-cap inflation cumulative index. Finally, divide the latter by the former and multiply it to uplift the incremental claims amount. Note that we divide here as this is a cumulative index. \n\nConsequently, now we have nominal incremental claims amount with valuation as at the year-end-cap.\n\nFor cases where we do not reach the equilibrium point, we will simply do nothing. Hence, the value remains.","metadata":{"_uuid":"63224f750256414e6afe165529ec4e916b9e2592"}},{"cell_type":"code","source":"# Uplift (Past Inflation) for Incremental Claims\npy_data['Inflated_Claims_Amount'] = py_data['Claims_Amount']\n\nfor row in range(0, len(py_data['Insured_Year'])):\n    InsuredYear = py_data.loc[row,'Insured_Year']\n    LagYear = py_data.loc[row,'Year_Only_Lag']\n    TransactionYear = InsuredYear + LagYear\n    for year in range(0, len(Inflation_df['Year'])):\n        CurrentYearInflation = Inflation_df.loc[year,'Year']\n        if  CurrentYearInflation == InsuredYear:\n            CurrentYearPerc = Inflation_df.loc[Inflation_df['Year'] == TransactionYear,'CumPastInflation']\n            ToYearPerc = Inflation_df.loc[Inflation_df['Year']==YearEndCap,'CumPastInflation'].values[0]\n            Uplift = ToYearPerc / CurrentYearPerc\n            py_data['Inflated_Claims_Amount'][row] = py_data['Inflated_Claims_Amount'][row]*Uplift\n        else:\n             py_data['Inflated_Claims_Amount'][row] = py_data['Inflated_Claims_Amount'][row]\n\nprint(py_data)","metadata":{"_uuid":"f0a155b902b76ad978441f078e60f5e79a56bf6b","_kg_hide-output":true,"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"**5.5**\tDerive corresponding uplifted Cumulative Amounts\n\nNow we simply use the 'cumsum' function to derive the corresponding inflated cumulative amounts.","metadata":{"_uuid":"3e57845ab3e4b71497c7059036adde53e86b45f5"}},{"cell_type":"code","source":"# Get Uplift (Past Inflation) Cumulative Claims\npy_data['Inflated_cumsum'] = py_data['Inflated_Claims_Amount'].groupby(py_data['Insured_Year']).cumsum()","metadata":{"_uuid":"8497e98464904fefe66c06b7004422c0eb31ba96","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"**5.6**\tIndividual Loss Development Factors\n\nHere we will now calculate each development factor (the multiple that resulted in the subsequent years cumulative claim amount) for each insured year. Reason being the IACL underlying assumption is that historical claim trends will follow suit. *Unhide to view output\n\nCode Explanation-\n\nFor each row in the \"py_data\" data-frame, we will retrieve the respective \"Insured year\", \"Lag year\", \"Transaction year\" and \"Current cumulative claims amount\".\n\nSubsequently, impose a dual 'either or' condition where if the \"Transaction year\" exceeds the year-end-cap or does not have a proceeding cumulative amount in the next \"Lag year\" we will have a zero LDF. Reason being this falls into the predicted year range which exceeds our past data range.\n\nCorrespondingly, upon not meeting this condition we will derive the \"Next cumulative claims amount\". We do this by simply looking up the same \"Insured year\" but adding 1 to the current \"Lag year\" (\"Lag year\" plus one).\n\nFinally, we divide the \"Next cumulative claims amount\" by the \"Current cumulative claims amount\" to derive the individual LDF and impute it.","metadata":{"_uuid":"e92c9043d2b57cff1456ee0729eb71b06c7de911"}},{"cell_type":"code","source":"# Inflated\npy_data['Inflated_LossDF'] = 1\n\nfor row in range(0, len(py_data['Insured_Year'])):\n    InsuredYear = py_data.loc[row, 'Insured_Year']\n    LagYr = py_data.loc[row, 'Year_Only_Lag']\n    CurrentYear = py_data.loc[row, 'Insured_Year'] + py_data.loc[row, 'Year_Only_Lag']\n    CurrCumAmt = py_data.loc[row, 'Inflated_cumsum']\n\n    if CurrentYear > YearEndCap or len(py_data.loc[(py_data['Insured_Year'] == InsuredYear) & (\n            py_data['Year_Only_Lag'] == (LagYr + 1)), 'Inflated_cumsum']) == 0:\n        NextCumAmt = 0\n    else:\n        NextCumAmt = py_data.loc[(py_data['Insured_Year'] == InsuredYear) & (\n                    py_data['Year_Only_Lag'] == (LagYr + 1)), 'Inflated_cumsum'].values[0]\n\n    LDF = NextCumAmt / CurrCumAmt\n    py_data.loc[row, 'Inflated_LossDF'] = LDF\n\nprint(py_data['Inflated_LossDF'])","metadata":{"_uuid":"e17cf0944c7b0ed74cb612083364cf9c6b88144d","_kg_hide-output":true,"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# Non Inflated\npy_data['LossDF'] = 1\n\nfor row in range(0, len(py_data['Insured_Year'])):\n    InsuredYear = py_data.loc[row, 'Insured_Year']\n    LagYr = py_data.loc[row, 'Year_Only_Lag']\n    CurrentYear = py_data.loc[row, 'Insured_Year'] + py_data.loc[row, 'Year_Only_Lag']\n    CurrCumAmt = py_data.loc[row, 'cumsum']\n\n    if CurrentYear > YearEndCap or len(py_data.loc[(py_data['Insured_Year'] == InsuredYear) & (\n            py_data['Year_Only_Lag'] == (LagYr + 1)), 'cumsum']) == 0:\n        NextCumAmt = 0\n    else:\n        # .values[0] code to output only values and not entire row\n        NextCumAmt = py_data.loc[\n            (py_data['Insured_Year'] == InsuredYear) & (py_data['Year_Only_Lag'] == (LagYr + 1)), 'cumsum'].values[0]\n\n    LDF = NextCumAmt / CurrCumAmt\n    py_data.loc[row, 'LossDF'] = LDF\n\nprint(py_data['LossDF'])","metadata":{"_uuid":"41361a15e241237cca0cd75f77ece75b5cd4dc04","_kg_hide-output":true,"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"* **6. Preview Triangles (Part B)**\n\nUntil this point we have only worked with past data. Now lets get a visual of what we have done so far.","metadata":{"_uuid":"fbc53c7bd5b1d6e590c350fd67c6589c0c616660"}},{"cell_type":"markdown","source":"**6.0**\tDefine General Plot Functions","metadata":{"_uuid":"c80be8e7b388f3ac165ba34f7692cb3cf67a14a8"}},{"cell_type":"code","source":"\"\"\"Claims Data - Single Plot\"\"\"\ndef SinglePlotPartialClaims(DataFrameName, InsuredYearColumn, LagYearColumn, ValueColumn):\n    import matplotlib.pyplot as plt\n    \"\"\"Create New df\"\"\"\n    Filtered_NewColumnNames = [\"Insured_Year\",\"Year_Only_Lag\",\"ClaimAmt\"]\n    Filtered_df = pd.DataFrame(DataFrameName[[InsuredYearColumn, LagYearColumn, ValueColumn]])\n    Filtered_df.columns = Filtered_NewColumnNames\n    \"\"\"Unique Insured Years List\"\"\"\n    InsuredYr_List = list(DataFrameName[InsuredYearColumn].unique())\n    \"\"\"Unique Lag Years List\"\"\"\n    LagYr_List = list(DataFrameName[LagYearColumn].unique())\n    \"\"\"Color List\"\"\"\n    ALL_Colors = ['r','b','g','y','k', 'c', 'm', 'saddlebrown', 'pink', 'lawngreen']         \n    Color_List = ALL_Colors[:len(InsuredYr_List)]\n    \"\"\"LineStyle List\"\"\"\n    ALL_LineStyle = ['-', '--', '-.', ':','-','-','-','-','-','-','-','-','-']\n    LineStyle_List = ALL_LineStyle[:len(InsuredYr_List)]\n    \"\"\"MarkerStyle List\"\"\"# First 4x empty \n    ALL_Markers = ['','','','','^','.','o','*', '+', '1', '2', '3', '4']\n    Marker_List = ALL_Markers[:len(InsuredYr_List)]\n    \"\"\"Loop Plot\"\"\"\n    for row_A in range(0,len(InsuredYr_List)):\n        plt.figure(2, figsize=(10,5))\n        Year_i = InsuredYr_List[row_A]\n        SubFiltered_df = Filtered_df.loc[Filtered_df['Insured_Year'].isin([Year_i])]\n        plt.plot(SubFiltered_df['Year_Only_Lag'], SubFiltered_df['ClaimAmt'], \n                 label=str(Year_i), linestyle='-', color=Color_List[row_A])\n    \"\"\"Plot Attributes\"\"\"    \n    plt.xlabel('Developement Year')\n    plt.ylabel('Claims Value')\n    plt.title('Single Plot Partial Claims Data')\n    plt.legend()\n    plt.show()","metadata":{"_uuid":"cacf09d0e5f238058afff715da79a73c2e1c4611","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"\"\"\"Claims Data - Sub Plot\"\"\"\ndef SubPlotPartialClaims(DataFrameName, InsuredYearColumn, LagYearColumn, ValueColumn):\n    import matplotlib.pyplot as plt\n    from matplotlib import rcParams\n    \"\"\"Create New df\"\"\"\n    Filtered_NewColumnNames = [\"Insured_Year\",\"Year_Only_Lag\",\"ClaimAmt\"]\n    Filtered_df = pd.DataFrame(DataFrameName[[InsuredYearColumn, LagYearColumn, ValueColumn]])\n    Filtered_df.columns = Filtered_NewColumnNames\n    \"\"\"Unique Insured Years List\"\"\"\n    InsuredYr_List = list(DataFrameName[InsuredYearColumn].unique())\n    \"\"\"Unique Lag Years List\"\"\"\n    LagYr_List = list(DataFrameName[LagYearColumn].unique())\n    \"\"\"Color List\"\"\"\n    ALL_Colors = ['r','b','g','y','k', 'c', 'm', 'saddlebrown', 'pink', 'lawngreen']         \n    Color_List = ALL_Colors[:len(InsuredYr_List)]\n    \"\"\"LineStyle List\"\"\"\n    ALL_LineStyle = ['-', '--', '-.', ':','-','-','-','-','-','-','-','-','-']\n    LineStyle_List = ALL_LineStyle[:len(InsuredYr_List)]\n    \"\"\"MarkerStyle List\"\"\"# First 4x empty \n    ALL_Markers = ['','','','','^','.','o','*', '+', '1', '2', '3', '4']\n    Marker_List = ALL_Markers[:len(InsuredYr_List)]\n    \"\"\"Plot Attributes\"\"\"\n    fig = plt.figure(2, figsize=(10,14))\n    plt.xticks([]) # remove initial blank plot default ticks\n    plt.yticks([]) # remove initial blank plot default ticks\n    plt.title('Sub Plot Partial Claims Data')\n    rcParams['axes.titlepad'] = 70 # position title\n    plt.box(on=None) # Remove boundary line\n    \"\"\"Loop Plot\"\"\"\n    i=0\n    for row_A in range(0,len(InsuredYr_List)):\n        ax = fig.add_subplot(5, 2, 1+i)\n        Year_i = InsuredYr_List[row_A]\n        SubFiltered_df = Filtered_df.loc[Filtered_df['Insured_Year'].isin([Year_i])]\n        plt.plot(SubFiltered_df['Year_Only_Lag'], SubFiltered_df['ClaimAmt'], \n                 label=str(Year_i), marker='o', linestyle='-', color=Color_List[row_A])\n        plt.xticks(np.arange(0, (YearEndCap-YearStartCap), step=1))\n        plt.legend()\n        i += 1\n        \"\"\"Plot Attributes\"\"\"\n        plt.xlabel('Developement Year')\n        plt.ylabel('Claims Value')\n    \n    fig.tight_layout() # set size\n    plt.show()","metadata":{"_uuid":"5e22f7e992a5863a4ef96c92844c3697daa95dc9","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"\"\"\"Loss Development Ratios\"\"\"\ndef SinglePlotLDF(DataFrameName, Columns):\n    import matplotlib.pyplot as  plt\n    \"\"\"Create New df\"\"\"\n    Filtered_df = pd.DataFrame(DataFrameName[Columns])    \n    \"\"\"Lag Years\"\"\"\n    LagYears_List = list(range(0, len(DataFrameName)))\n    \"\"\"Color List\"\"\"\n    ALL_Colors = ['r','b','g','y','k', 'c', 'm', 'saddlebrown', 'pink', 'lawngreen']         \n    Color_List = ALL_Colors[:len(Columns)]\n    \"\"\"Loop Plot\"\"\"\n    plt.figure(2, figsize=(10,5))\n    for row_A in range(0,len(Columns)):\n        Column_i = Columns[row_A]\n        plt.plot(LagYears_List, Filtered_df[Column_i], label=str(Column_i), linestyle='-', color=Color_List[row_A])\n        plt.legend()         \n    \"\"\"Plot Attributes\"\"\"    \n    plt.xlabel('Developement Year')\n    plt.ylabel('Ratio')\n    plt.title('Loss Development Factors')\n    plt.show()","metadata":{"_uuid":"b02cd73ea7717215fdf9da6d076da09c03616bb0","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"**6.1**\tIncremental Amount\n        ","metadata":{"_uuid":"68d844a721c4aa796fd04d424c3371b562768c77"}},{"cell_type":"markdown","source":"# Inflated Incremental","metadata":{"_uuid":"dc86054f8b361c732d3033fd9514f13cbebc4f01"}},{"cell_type":"code","source":"# Incremental Claims Amount\n# Inflated\npy_triangle_inflated = pd.pivot_table(py_data, index=[\"Insured_Year\"], columns=[\"Year_Only_Lag\"], values=[\"Inflated_Claims_Amount\"])\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nsns.distplot(py_data['Inflated_Claims_Amount'], kde=False, fit=stats.lognorm) # norm, pareto, loggamma, gompertz\nplt.show()\ndisplay(py_triangle_inflated)","metadata":{"_uuid":"40fa6da5f796a7bf370b53bdace44f82a2ce036c","trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# Non-Inflated Incremental","metadata":{"_uuid":"05a9a712345c9762057c298d8a3a453abbef80da"}},{"cell_type":"code","source":"# Incremental Claims Amount\n# Non-Inflated\npy_triangle = pd.pivot_table(py_data, index=[\"Insured_Year\"], columns=[\"Year_Only_Lag\"], values=[\"Claims_Amount\"])\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nsns.distplot(py_data['Claims_Amount'], kde=False, fit=stats.lognorm) # norm, pareto, loggamma, gompertz\nplt.show()\ndisplay(py_triangle)","metadata":{"_uuid":"3b06176a2e77a2722ba86507a1bbaeeae2bb5cdb","trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"**6.2**\tCumulative Amounts","metadata":{"_uuid":"7351141bb6561916dc9168ceec3e6819f005021b"}},{"cell_type":"markdown","source":"# Inflated Cumulative","metadata":{"_uuid":"5d3eac1e0629fa63870f74593aa27d3394860cdf"}},{"cell_type":"code","source":"# Cumulative Claims Amount\n# Inflated\npy_triangle_cum_inflated = pd.pivot_table(py_data, index=[\"Insured_Year\"], columns=[\"Year_Only_Lag\"], values=[\"Inflated_cumsum\"])\nSinglePlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='Inflated_cumsum')\nSubPlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='Inflated_cumsum')\ndisplay(py_triangle_cum_inflated)","metadata":{"_uuid":"978625035490e1d7e990cf99ee7d79db5048aed7","trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"# Non-Inflated Cumulative","metadata":{"_uuid":"7d5e2c9ed3a791f40c279e39c8d93f979a9c1a62"}},{"cell_type":"code","source":"# Cumulative Claims Amount\n# Non-Inflated\npy_triangle_cum = pd.pivot_table(py_data, index=[\"Insured_Year\"], columns=[\"Year_Only_Lag\"], values=[\"cumsum\"])\nSinglePlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='cumsum')\nSubPlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='cumsum')\ndisplay(py_triangle_cum)","metadata":{"_uuid":"261bd3a9e79e0b1dcc96a4a95c9395210afb3b6b","trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"**6.3**\tIndividual Loss Development Factors","metadata":{"_uuid":"314c60c97085b46f90640a3318396dc721e998c8"}},{"cell_type":"markdown","source":"# Inflated LDF","metadata":{"_uuid":"a1f93373d2e7a023dad5d205c7d28cbb9f278694"}},{"cell_type":"code","source":"# Individual Loss Development factors\n# Inflated\npy_InflatedLossDF_triangle = pd.pivot_table(py_data, index=[\"Insured_Year\"], columns=[\"Year_Only_Lag\"], values=[\"Inflated_LossDF\"])\nSinglePlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='Inflated_LossDF')\nSubPlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='Inflated_LossDF')\ndisplay(py_InflatedLossDF_triangle)","metadata":{"_uuid":"e9327a73ea8c778d5ecf92eee265ffb18df9993a","trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"# Non-Inflated LDF","metadata":{"_uuid":"ad3ff63e7b6fd04a3a79b937c9f3c27b3d8163c0"}},{"cell_type":"code","source":"# Individual Loss Development factors\n# Non-Inflated\npy_LossDF_triangle = pd.pivot_table(py_data, index=[\"Insured_Year\"], columns=[\"Year_Only_Lag\"], values=[\"LossDF\"])\nSinglePlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='LossDF')\nSubPlotPartialClaims(DataFrameName=py_data, InsuredYearColumn='Insured_Year', LagYearColumn='Year_Only_Lag', ValueColumn='LossDF')\ndisplay(py_LossDF_triangle)","metadata":{"_uuid":"7b3ac10b8aa3775bd762862965d77253f68dd078","trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"* **7. Calculations (Part B)**","metadata":{"_uuid":"bc5faa1a12ddccee6c21e40374fa7fea662fe661"}},{"cell_type":"markdown","source":"**7.1**\tEstablish Predicted_df\n\nWe will first create a temporary dummy data-frame \"temp_df\" containing all the years and lag years that are within our data range for analysis only for referencing purposes. \n\n","metadata":{"_uuid":"ea6b72ba8bcb111cc4c81b7a9ab767df8648b661"}},{"cell_type":"code","source":"# Create a Temp Df of Predicted Years & LagYears rates\ncolumns_3 = ['InsuredYear', 'PredictedYear_Only_Lag',\n             'Previous_cumsum', 'Predicted_cumsum', 'Predicted_Incremental',\n             'Previous_Inflated_cumsum', 'Predicted_Inflated_cumsum', 'Predicted_Inflated_Incremental']\nTemp_df = pd.DataFrame(columns=columns_3)\n# +1 due to 31 Dec 2017 (also not a Bday) & +1 due to range exlusion of last value cap\nInsuredYr = list(range(YearStartCap + 1, YearEndCap + 1, 1))  # [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\nTemp_df['InsuredYear'] = InsuredYr\nLags = list(range(0, YearEndCap - YearStartCap, 1))  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nTemp_df['PredictedYear_Only_Lag'] = Lags\n\n# Establish Predicted data-frame\nPredicted_df = pd.DataFrame(columns=columns_3)","metadata":{"_uuid":"00ee19b3101f2d8760e6e045c66826f499e3a565","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"**7.2**\tCoordinates (InsuredYear by LagYr) of predicted cells\n\nWe will now move on to create the actual data-frame of the corresponding predicted insured years and lag years named as \"Predicted_df\". In other words, it is simply the combination of both the \"Insured_Years\" and \"Lag Years\" for all the NaNs that we have in the claims triangle seen before. *Unhide to view output\n\nCode Explanation-\n\nThe code in short executes 2 nested loops to compare each \"Transaction Year\" (or \"Insured_Years\" plus \"Lag Year\") against the year-end-cap. Subsequently, impute the corresponding \"Insured_Years\" and \"Predicted Lag Years\". This will then act as the coordinates for the NaNs which we will use later as references for imputing our predictions.\n\nThe first loop iterates through each \"InsuredYear\" column in the \"Temp_df\" data-frame to establish the \"InsuredYear\".\n\nFrom here, for each \"InsuredYear\" we will now start a second loop to iterate through the \"Lag Years\" and derive the \"P_yr\" (or also \"Transaction Year\").\n\nNow we will set a condition, where if the \"P_yr\" exceeds the year-end-cap we will impute that \"InsuredYear\" and \"Lag Year\" combination. Reason being because it falls beyond our past data range hence it is a predicted year. Intuitively, just the opposite of what was done in calculating the individual LDFs.\n\nNot meeting this condition, we will do nothing. Do also note the i=0 and i += 1 is for indexing purposes. 0 as python data-frames defaults via a 0 index at outset and +1 to move to the next row after imputing.","metadata":{"_uuid":"5bf5b616923117e70798e080dc3aa8f0438a0916"}},{"cell_type":"code","source":"# Coordinates of predicted Insured Years & Lag Years\nx = 1 # Do nothing\ni = 0 # For loop impute indexing\nfor row in range(0, len(Temp_df['InsuredYear'])):\n    BaseYr = Temp_df.loc[row, 'InsuredYear']\n    for lag in range(0, len(Temp_df['PredictedYear_Only_Lag'])):\n        LagYr = Temp_df.loc[lag, 'PredictedYear_Only_Lag']\n        P_yr = BaseYr + Temp_df.loc[lag, 'PredictedYear_Only_Lag']\n        if P_yr > YearEndCap:\n\n            Predicted_df.loc[i, 'InsuredYear'] = BaseYr\n            Predicted_df.loc[i, 'PredictedYear_Only_Lag'] = LagYr\n            i += 1\n        else:\n            x = x\n\nprint(Predicted_df[['InsuredYear', 'PredictedYear_Only_Lag']])","metadata":{"_uuid":"c2acc3d7362a1666d0ff4d2e38b8590fdb7500fe","_kg_hide-output":true,"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"**7.3**\tImpute latest Cumulative Amounts available (as a base point for multiplying by LDF)\n\nThe reason for this is because we need a base point for multiplying by our LDFs and predicting future cumulative claim amounts later. Rather than referencing separatly, we will simply do a look-up and impute accordingly. It neatens the process. *Unhide to view output\n\nCode Explanation-\n\nIn short, the code loops through the \"Insured year\" and \"Lag year\" columns in \"Predicted_df\" that we derived earlier and uses these two references as look-up references against the \"py_data\" (containing past data) to find and impute the corresponding latest cumulative amounts available for that respective insured year.\n\nThe loop and first 3 code lines establishes the \"Insured year\", \"Lag year\" and \"PredYr\" (\"Insured year\" + \"Lag year\") for each respective loop iteration while in the \"Predicted_df\" DataFrame.\n\nWe then set 3 levels of 'If' conditionals to determine the latest cumulative sum.\n\nIn chronological order-\n\nFirst condition; if the \"Insured year\" is equivalent to the year-end-cap, there is only one previous cumulative sum (which is the value at the lowest bottom point of a claim triangle). Hence, we will only output that.\n\nSecond condition; if the predicted year \"PredYr\" exceeds the year-end-cap or if the look-up reference (via the same \"Insured year\" and \"Lag year\" minus one) for the previous cumulative sum renders none, we will keep the same insured year but replace the \"Lag year\" minus one formulae. Instead take the \"Maximum Lag year\" (the lag year of the latest cumulative claim amount available in that insured year) for that insured year as reference for the cumulative amount look-up.\n\nTo put it contextually, if you refer to the above PART-5 Raw Preliminary view of Claims Triangle it is the \"Year_Only_Lag\" numbers just before a NaN. For \"Insured Year\"-2017 it would be \"Year_Only_Lag\"-0, for \"Insured Year\"-2016 it would be \"Year_Only_Lag\"-1.... etc\n\nThird condition; the residual of which does not fulfil the above two conditions uses this. Where we will simply execute the cumulative sum look-up references via the same \"Insured year\" and \"Lag year\" minus one as a reference.\n\nFinally, we will impute the latest cumulative sum in the column \"Previous_Inflated_cumsum\" of that respective row iteration in the loop within the \"Predicted_df\" data-frame.\n","metadata":{"_uuid":"8289263fbc701befefb3a4b55d747ee702bdd255"}},{"cell_type":"code","source":"# Impute latest cumulative amounts available\n# Inflated\nfor row in range(0, len(Predicted_df)):\n    Base = Predicted_df.loc[row, 'InsuredYear']\n    Lag = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    PredYr = Base + Lag\n\n    if Base == YearEndCap:\n        PrevInflatedCumSum = py_data.loc[(py_data['Insured_Year'] == Base), 'Inflated_cumsum'].values[0]\n\n    else:\n        if PredYr > YearEndCap or len(py_data.loc[(py_data['Insured_Year'] == Base) & (py_data['Year_Only_Lag'] == Lag - 1), 'Inflated_cumsum']) == 0:\n            MaxLag = py_data.loc[(py_data['Insured_Year'] == Base), 'Year_Only_Lag'].max()\n            PrevInflatedCumSum = py_data.loc[(py_data['Insured_Year'] == Base) & (py_data['Year_Only_Lag'] == MaxLag), 'Inflated_cumsum'].values[0]\n\n        else:\n            PrevInflatedCumSum = py_data.loc[(py_data['Insured_Year'] == Base) & (py_data['Year_Only_Lag'] == Lag - 1), 'Inflated_cumsum'].values[0]\n\n    Predicted_df.loc[row, 'Previous_Inflated_cumsum'] = PrevInflatedCumSum\n\nprint(Predicted_df['Previous_Inflated_cumsum'])","metadata":{"_uuid":"0867b09bb3e6f76a92bbb0e1fd3cc74968d4c3e4","_kg_hide-output":true,"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Impute latest cumulative amounts available\n# Non-Inflated\nfor row in range(0, len(Predicted_df)):\n    Base = Predicted_df.loc[row, 'InsuredYear']\n    Lag = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    PredYr = Base + Lag\n\n    if Base == YearEndCap:\n        PrevCumSum = py_data.loc[(py_data['Insured_Year'] == Base), 'cumsum'].values[0]\n\n    else:\n        if PredYr > YearEndCap or len(\n                py_data.loc[(py_data['Insured_Year'] == Base) & (py_data['Year_Only_Lag'] == Lag - 1), 'cumsum']) == 0:\n            MaxLag = py_data.loc[(py_data['Insured_Year'] == Base), 'Year_Only_Lag'].max()\n            PrevCumSum = py_data.loc[(py_data['Insured_Year'] == Base) & (py_data['Year_Only_Lag'] == MaxLag), 'cumsum'].values[0]\n\n        else:\n            PrevCumSum = py_data.loc[(py_data['Insured_Year'] == Base) & (py_data['Year_Only_Lag'] == Lag - 1), 'cumsum'].values[0]\n\n    Predicted_df.loc[row, 'Previous_cumsum'] = PrevCumSum\n    \nprint(Predicted_df['Previous_cumsum'])","metadata":{"_uuid":"4e5a6b22b637b3bb303603c0ceadbb8260b97407","_kg_hide-output":true,"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"**7.4**\tSimpleMeanLoss & Volume Weighted & Last 5/3 years & Selected LDF\n\nNow with the individual LDFs calculated earlier in *Chapter 5.6* we will now derive the average-lag year-to-lag-year LDFs. ","metadata":{"_uuid":"7fa5e41dcd6a581aaf4c651b2f879123a1884f35"}},{"cell_type":"markdown","source":"We first establish the initial data-frame columns and corresponding year lags for each average LDFs to reference against when calculating from the individual LDFs.","metadata":{"_uuid":"23ee01a8d8c020cb657a5638eea26bfeb2499ce4"}},{"cell_type":"code","source":"# Establish averaged-year-to-year LDF\ncolumns_4 = ['Year_Only_Lag',\n             'SimpleMeanLossDF', 'VolWtdLossDF',\n             'CumToUlt_SimpleMeanLossDF', 'CumToUlt_VolWtdLossDF',\n             'SimpleMeanLossDF_5year', 'VolWtdLossDF_5year',\n             'SimpleMeanLossDF_3year', 'VolWtdLossDF_3year',\n             'SelectLossDF'\n             'Inflated_SimpleMeanLossDF', 'Inflated_VolWtdLossDF',\n             'Inflated_CumToUlt_SimpleMeanLossDF', 'Inflated_CumToUlt_VolWtdLossDF',\n             'Inflated_SimpleMeanLossDF_5year', 'Inflated_VolWtdLossDF_5year',\n             'Inflated_SimpleMeanLossDF_3year', 'Inflated_VolWtdLossDF_3year',\n             'Inflated_SelectLossDF']\nLossDF_df = pd.DataFrame(columns=columns_4)\nLags = list(range(0, YearEndCap-YearStartCap, 1))\nLossDF_df['Year_Only_Lag'] = Lags\ndisplay(LossDF_df)","metadata":{"_uuid":"8902c2fe2078415295f0e06dda87996540c4bf0c","trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"Code Explanation-\n\nThe code loops though each \"Lag Years\" (\"Year_Only_Lag\" column) in the \"LossDF_df\" data-frame to reference the required lag year. Subsequently, for each \"Lag Year\" executes the simple mean and volume weight LDF calculations below.\n\nSimple Mean - Looks up all the LDFs having that required \"Lag Year\" reference and takes the average. However, excluding the last LDF as that would be the LDF for moving into a year outside our data range. In other words, a predicted year.\n\nVolume Weighted - Looks up all the cumulative sums having that required lag year and also the subsequent next lag year (required lag year plus 1 year) and takes the sum. However, for that required year it excludes the last cumulative sum to ensure equitable quantity of summing components. Just as in the simple mean calculation.\n\nFinally, impute and +1 to move to the next row for the proceeding loop to impute accordingly.","metadata":{"_uuid":"1fca3d8d1cd86a19dac008a14ca38505e6e997f6"}},{"cell_type":"markdown","source":"# Inflated All Year Average LDFs","metadata":{"_uuid":"11a2f1961047e60ed6f0cefca3df75fb06a41c7e"}},{"cell_type":"code","source":"# Inflated\ni=0\nfor lag in range(0, len(Temp_df['PredictedYear_Only_Lag'])):\n    lagyr = Temp_df.loc[lag, 'PredictedYear_Only_Lag']\n    # Simple Mean\n    # due to 0 input so exlude last value\n    SimpleMeanLossDF = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_LossDF'][:-1].mean()\n    LossDF_df.loc[i, 'Inflated_SimpleMeanLossDF'] = SimpleMeanLossDF\n    # Volume Weighted\n    Deno = py_data.loc[py_data['Year_Only_Lag'] == (lagyr + 1), 'Inflated_cumsum'].sum()\n    Neum = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_cumsum'][:-1].sum()\n    VolWtdLossDF = Deno / Neum\n    LossDF_df.loc[i, 'Inflated_VolWtdLossDF'] = VolWtdLossDF\n    i += 1\n\n# [::-1] to flip or invert the row order\nLossDF_df['Inflated_CumToUlt_SimpleMeanLossDF']=LossDF_df['Inflated_SimpleMeanLossDF'][::-1].cumprod()\nLossDF_df['Inflated_CumToUlt_VolWtdLossDF']=LossDF_df['Inflated_VolWtdLossDF'][::-1].cumprod()\n\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['Inflated_SimpleMeanLossDF', 'Inflated_VolWtdLossDF'])\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['Inflated_CumToUlt_SimpleMeanLossDF', 'Inflated_CumToUlt_VolWtdLossDF'])\ndisplay(LossDF_df[['Inflated_SimpleMeanLossDF', 'Inflated_VolWtdLossDF', 'Inflated_CumToUlt_SimpleMeanLossDF', 'Inflated_CumToUlt_VolWtdLossDF']])","metadata":{"_uuid":"e690d85c791a6e1a6adf2ec396c844acabcac6de","trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"# Non-Inflated All Year Average LDFs","metadata":{"_uuid":"2fa3df933ac1174d7f136a4691d72c32f677bde6"}},{"cell_type":"code","source":"# Non-Inflated\ni=0\nfor lag in range(0, len(Temp_df['PredictedYear_Only_Lag'])):\n    lagyr = Temp_df.loc[lag, 'PredictedYear_Only_Lag']\n    # Simple Mean\n    # due to 0 input so exlude last value\n    SimpleMeanLossDF = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'LossDF'][:-1].mean()\n    LossDF_df.loc[i, 'SimpleMeanLossDF'] = SimpleMeanLossDF\n    # Volume Weighted\n    Deno = py_data.loc[py_data['Year_Only_Lag'] == (lagyr + 1), 'cumsum'].sum()\n    Neum = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'cumsum'][:-1].sum()\n    VolWtdLossDF = Deno / Neum\n    LossDF_df.loc[i, 'VolWtdLossDF'] = VolWtdLossDF\n    i += 1\n\n# [::-1] to flip or invert the row order\nLossDF_df['CumToUlt_SimpleMeanLossDF']=LossDF_df['SimpleMeanLossDF'][::-1].cumprod()\nLossDF_df['CumToUlt_VolWtdLossDF']=LossDF_df['VolWtdLossDF'][::-1].cumprod()\n\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['SimpleMeanLossDF', 'VolWtdLossDF'])\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['CumToUlt_SimpleMeanLossDF', 'CumToUlt_VolWtdLossDF'])\ndisplay(LossDF_df[['SimpleMeanLossDF', 'VolWtdLossDF', 'CumToUlt_SimpleMeanLossDF', 'CumToUlt_VolWtdLossDF']])","metadata":{"_uuid":"f753abfe3d12857a7a5df9c9bc355cd225989981","trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"Last 5 & 3 year averages\n\nThe 5/3 Year Averages are just as we did earlier. The only difference is that now rather than the '-1' to exclude the final entry, we replace that with the number of years we want. In this case, I declared them as Year_A for 5 year and Year_B for 3 year averages.","metadata":{"_uuid":"47da3ae7ba859c89d9a708f6dd6264e6574a9d28"}},{"cell_type":"markdown","source":"# Inflated 5 & 3 year Average LDFs","metadata":{"_uuid":"a33cabb94047e1b6dcef912946952df817880f89"}},{"cell_type":"code","source":"# Inflated\ni=0\nfor lag in range(0, len(Temp_df['PredictedYear_Only_Lag'])):\n    lagyr = Temp_df.loc[lag, 'PredictedYear_Only_Lag']\n    # Simple Mean\n    Year_A = 5   # 5 Year\n    SimpleMeanLossDF_Ayear = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_LossDF'][:Year_A].mean()\n    LossDF_df.loc[i, 'Inflated_SimpleMeanLossDF_5year'] = SimpleMeanLossDF_Ayear\n    Year_B = 3   # 3 Year\n    SimpleMeanLossDF_Byear = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_LossDF'][:Year_B].mean()\n    LossDF_df.loc[i, 'Inflated_SimpleMeanLossDF_3year'] = SimpleMeanLossDF_Byear\n    # Volume Weighted\n    Deno_A = py_data.loc[py_data['Year_Only_Lag'] == (lagyr + 1), 'Inflated_cumsum'][:Year_A].sum()\n    Neum_A = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_cumsum'][:Year_A].sum()\n    VolWtdLossDF_A = Deno_A / Neum_A\n    LossDF_df.loc[i, 'Inflated_VolWtdLossDF_5year'] = VolWtdLossDF_A\n    Deno_B = py_data.loc[py_data['Year_Only_Lag'] == (lagyr + 1), 'Inflated_cumsum'][:Year_B].sum()\n    Neum_B = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_cumsum'][:Year_B].sum()\n    VolWtdLossDF_B = Deno_B / Neum_B\n    LossDF_df.loc[i, 'Inflated_VolWtdLossDF_3year'] = VolWtdLossDF_B\n    i += 1\n\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['Inflated_SimpleMeanLossDF_5year', 'Inflated_VolWtdLossDF_5year'])\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['Inflated_SimpleMeanLossDF_3year', 'Inflated_VolWtdLossDF_3year'])    \ndisplay(LossDF_df[['Inflated_SimpleMeanLossDF_5year', 'Inflated_VolWtdLossDF_5year', 'Inflated_SimpleMeanLossDF_3year', 'Inflated_VolWtdLossDF_3year']])","metadata":{"_uuid":"cd95303a65dbc4a755066083380a4a1f493a944b","trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"# Non-Inflated 5 & 3 year Average LDFs","metadata":{"_uuid":"20908b5df08acc3cbbef794d1cd0225b052136d7"}},{"cell_type":"code","source":"# Non Inflated\ni=0\nfor lag in range(0, len(Temp_df['PredictedYear_Only_Lag'])):\n    lagyr = Temp_df.loc[lag, 'PredictedYear_Only_Lag']\n    # Simple Mean\n    Year_A = 5   # 5 Year\n    SimpleMeanLossDF_Ayear = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_LossDF'][:Year_A].mean()\n    LossDF_df.loc[i, 'SimpleMeanLossDF_5year'] = SimpleMeanLossDF_Ayear\n    Year_B = 3   # 3 Year\n    SimpleMeanLossDF_Byear = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_LossDF'][:Year_B].mean()\n    LossDF_df.loc[i, 'SimpleMeanLossDF_3year'] = SimpleMeanLossDF_Byear\n    # Volume Weighted\n    Deno_A = py_data.loc[py_data['Year_Only_Lag'] == (lagyr + 1), 'Inflated_cumsum'][:Year_A].sum()\n    Neum_A = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_cumsum'][:Year_A].sum()\n    VolWtdLossDF_A = Deno_A / Neum_A\n    LossDF_df.loc[i, 'VolWtdLossDF_5year'] = VolWtdLossDF_A\n    Deno_B = py_data.loc[py_data['Year_Only_Lag'] == (lagyr + 1), 'Inflated_cumsum'][:Year_B].sum()\n    Neum_B = py_data.loc[py_data['Year_Only_Lag'] == lagyr, 'Inflated_cumsum'][:Year_B].sum()\n    VolWtdLossDF_B = Deno_B / Neum_B\n    LossDF_df.loc[i, 'VolWtdLossDF_3year'] = VolWtdLossDF_B\n    i += 1\n\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['SimpleMeanLossDF_5year', 'VolWtdLossDF_5year'])\nSinglePlotLDF(DataFrameName=LossDF_df, Columns=['SimpleMeanLossDF_3year', 'VolWtdLossDF_3year'])\ndisplay(LossDF_df[['SimpleMeanLossDF_5year', 'VolWtdLossDF_5year', 'SimpleMeanLossDF_3year', 'VolWtdLossDF_3year']])","metadata":{"_uuid":"28bd57c81b7acbb6598e32d4cc54656272b931f4","trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"Selected LDF\n\nIn real world scenarios, actuaries will analyze the consistency in LDFs calculated above. Some reasons include abnormally large claims may distort the LDF trend, legislative reasons to exclude a specific number of years in the LDF averages etc..\n\nIn this case we will simply use the fully all year averaged LDFs since it is the smoothest amongst the choices.","metadata":{"_uuid":"53af52a0757f45ab5cfa8cd5d1bc199d060b378c"}},{"cell_type":"code","source":"LossDF_df['Inflated_SelectLossDF'] = LossDF_df['Inflated_VolWtdLossDF']\nLossDF_df['SelectLossDF'] = LossDF_df['VolWtdLossDF']","metadata":{"_uuid":"eee37005705432e5dedb710b3f018c6f82bb7717","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"**7.5**\tPredicted Cumulative Amounts = Uplift previous Cumulative Amounts by LDF\n\nNow with a selected LDF we will proceed to use historical claim trends to predict future claim trends! Pretty much self explanatory here. We simply apply the respective LDF to each past cumulative claim to derive the future cumulative claim. On the assumption trends will follow suit. *Unhide to view output\n\nCode Explanation-\n\nAs mentioned before we are predicting using the latest cumulative amount as the baseline. Thus, we will set them equal first for easy reference.\n\nIn short, the code iterates through the \"Predicted_df\" to determine the year range to apply the LDF for prediction, and correspondingly references the LDF aligning to that year range from the \"LossDF_df\" to predict the amount.\n\nThe code uses 2 nested loops. The first loop in the \"Predicted_df\" is used to establish the combination of the \"Insured year\" and \"Lag year\" that the predicted year belongs to. It is also used to derive the \"Maximum Lag year\". Exactly, what we did in PART-7 Impute latest cumulative amounts.\n\nThe second loop in the \"LossDF_df\" data-frame is used to iterate over the various \"Lag years\" to reference the averaged-by-lag-year LDFs we calculated above to predict the respective cumulative amounts. Whilst under the second loop, we set 2 conditionals -\n\nFirst condition: If this ongoing second loop iteration reaches the last \"Lag year\" we will do nothing. The reason being is that the final lag year is the ultimate \"Lag year\", hence no LDF is available.\n\nSecond condition: If this second loop iteration's \"Lag year\" reaches a equilibrium with the maximum \"Lag year\" for that \"Insured year\" from that ongoing first loop iteration we will only then proceed with the predicting calculation. In other words, when the \"Lag year\" of the \"Predicted_df\" and the \"LossDF_df\" are equal.\n\nThe calculation simply takes the product of all the averaged-by-lag-year LDFs falling within inclusively of the maximum lag year and the predicted lag year minus one range (both of which established from that ongoing first loop iteration) and multiplies that into the latest cumulative sum to attain the predicted cumulative sum.\n\nNot fulfilling either condition we will do nothing.\n","metadata":{"_uuid":"9e24306a2ef042fdb09cb6f1a4bb8b4a2461185d"}},{"cell_type":"code","source":"# Predict Cumulative Claim Amounts\n# Inflated\n# Set Equal for easy reference\nPredicted_df['Predicted_Inflated_cumsum'] = Predicted_df['Previous_Inflated_cumsum']\nlagyearlimit = (YearEndCap - YearStartCap) - 1\nx = 1  # Do nothing\nfor row in range(0, len(Predicted_df)):\n    PredLagYr = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    BaseInsuredYr = Predicted_df.loc[row, 'InsuredYear']\n    MaxLagYr = py_data.loc[(py_data['Insured_Year'] == BaseInsuredYr), 'Year_Only_Lag'].max()\n    for r in range(0, len(LossDF_df)):\n        if (LossDF_df.loc[r, 'Year_Only_Lag'] == lagyearlimit):\n            x = x  # To avoid NaN\n        elif (LossDF_df.loc[r, 'Year_Only_Lag'] == MaxLagYr):\n            # LDF multiplication\n            LDF = LossDF_df.loc[(LossDF_df['Year_Only_Lag'] >= MaxLagYr) & (LossDF_df['Year_Only_Lag'] <= (PredLagYr - 1)), 'Inflated_SelectLossDF'].prod()\n            Predicted_df.loc[row, 'Predicted_Inflated_cumsum'] = Predicted_df.loc[row, 'Predicted_Inflated_cumsum'] * LDF\n        else:\n            x = x  # Do nothing\n            \nprint(Predicted_df['Predicted_Inflated_cumsum'])","metadata":{"_uuid":"a25fad244c1e75204b1a7b6c0ff364948f28f318","_kg_hide-output":true,"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# Predict Cumulative Claim Amounts\n# Non-Inflated\n# Set Equal for easy reference\nPredicted_df['Predicted_cumsum'] = Predicted_df['Previous_cumsum']\nlagyearlimit = (YearEndCap - YearStartCap) - 1\nx = 1  # Do nothing\nfor row in range(0, len(Predicted_df)):\n    PredLagYr = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    BaseInsuredYr = Predicted_df.loc[row, 'InsuredYear']\n    MaxLagYr = py_data.loc[(py_data['Insured_Year'] == BaseInsuredYr), 'Year_Only_Lag'].max()\n    for r in range(0, len(LossDF_df)):\n        if (LossDF_df.loc[r, 'Year_Only_Lag'] == lagyearlimit):\n            x = x  # To avoid NaN\n        elif (LossDF_df.loc[r, 'Year_Only_Lag'] == MaxLagYr):\n            # LDF multiplication\n            LDF = LossDF_df.loc[(LossDF_df['Year_Only_Lag'] >= MaxLagYr) & (LossDF_df['Year_Only_Lag'] <= (PredLagYr - 1)), 'SelectLossDF'].prod()\n            Predicted_df.loc[row, 'Predicted_cumsum'] = Predicted_df.loc[row, 'Predicted_cumsum'] * LDF\n        else:\n            x = x  # Do nothing\n\nprint(Predicted_df['Predicted_cumsum'])","metadata":{"_uuid":"9a6e4fea8daee2fa3dceef03cdf818c5adc18ed3","_kg_hide-output":true,"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"**7.6**\tData-type adjustments (int & float)\n\nThis is just a intermediate step to ensure consistent computational data-types as so many data manipulations were made before.","metadata":{"_uuid":"c3704faa0f76955a4d178b200809bc447d2742a0"}},{"cell_type":"code","source":"# Data-type adjustments\n# Years\nPredicted_df[['InsuredYear','PredictedYear_Only_Lag']]=Predicted_df[['InsuredYear','PredictedYear_Only_Lag']].astype(int)\n# Amounts\nPredicted_df[['Predicted_cumsum','Previous_cumsum']]=Predicted_df[['Predicted_cumsum','Previous_cumsum']].astype(float)\nPredicted_df[['Predicted_Inflated_cumsum','Previous_Inflated_cumsum']]=Predicted_df[['Predicted_Inflated_cumsum','Previous_Inflated_cumsum']].astype(float)","metadata":{"_uuid":"35f0b9c5e26e448e70f1a56641c910daec71d092","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"**7.7**\tPredicted Incremental Amount\n\n**Do note that the Insured year column now starts from 2009 not 2008 as in the past claims data \"py_data\" data-frame**\n\nWith the predicted cumulative amount derive earlier, we will now derive the incremental amount. Reason being we need to use the incremental amount to project future inflation (just as we did for past inflation).\n\nCode Explanation-\n\nThe code simply loops through both data-frames \"Predicted_df\" (predicted data) and \"py_data\" (past data) and looks up the respective current and previous cumulative amount based on \"Insured Year\" and \"Lag Year\" references. Subsequently, calculates the difference which is then the incremental amount.\n\nJust as before we will loop the predicted data-frame and first establish the \"Insured year\", \"Lag year\" and \"Current predicted cumulative amount\" (belonging to the current loop iteration). After which using the \"Insured year\" and \"Lag year\" minus one combination as references to look-up the previous cumulative amount.\n\nThe code then sets 2 conditionals -\n\nFirst condition: If we are not able to look up the respective previous cumulative values in the predicted data-frame \"Predicted_df\", we instead search in the past cumulative values data-frame \"py_data\". This specifically required for those amounts falling on the 'steps' of a claim triangle.\n\nSecond condition: Here we are simply searching the predicted data-frame \"Predicted_df\".","metadata":{"_uuid":"f5432e0445e0385a604d93c8e6558ad1758f74aa"}},{"cell_type":"code","source":"# Predict Incremental Amount\n# Inflated\nfor row in range(0, len(Predicted_df)):\n    InsurYr = Predicted_df.loc[row, 'InsuredYear']\n    LagYr = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    CurrCum = Predicted_df.loc[row, 'Predicted_Inflated_cumsum']\n    # For which we can't look up in Predicted_df\n    if len(Predicted_df.loc[(Predicted_df['InsuredYear'] == InsurYr) & (Predicted_df['PredictedYear_Only_Lag'] == LagYr - 1), 'Predicted_Inflated_cumsum']) == 0:\n        PrevCum = py_data.loc[(py_data['Insured_Year'] == InsurYr) & (py_data['Year_Only_Lag'] == LagYr - 1), 'Inflated_cumsum'].values[0]\n    # For which we can look up in Predicted_df\n    else:\n        PrevCum = Predicted_df.loc[(Predicted_df['InsuredYear'] == InsurYr) & (Predicted_df['PredictedYear_Only_Lag'] == LagYr - 1), 'Predicted_Inflated_cumsum'].values[0]\n\n    Predicted_df.loc[row, 'Predicted_Inflated_Incremental'] = (CurrCum - PrevCum)\n\nPredicted_df[['Predicted_Inflated_Incremental']] = Predicted_df[['Predicted_Inflated_Incremental']].astype(float)\nPredictedInflatedIncrementalTriangle = pd.pivot_table(Predicted_df, index=[\"InsuredYear\"],columns=[\"PredictedYear_Only_Lag\"],values=[\"Predicted_Inflated_Incremental\"])\n\n# print(PredictedInflatedIncrementalTriangle)\ndisplay(PredictedInflatedIncrementalTriangle)","metadata":{"_uuid":"2a84759ec19e853189166554934477e7ad4478ba","trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# Predict Incremental Amount\n# Non-Inflated\nfor row in range(0, len(Predicted_df)):\n    InsurYr = Predicted_df.loc[row, 'InsuredYear']\n    LagYr = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    CurrCum = Predicted_df.loc[row, 'Predicted_cumsum']\n\n    if len(Predicted_df.loc[(Predicted_df['InsuredYear'] == InsurYr) & (Predicted_df['PredictedYear_Only_Lag'] == LagYr - 1), 'Predicted_cumsum']) == 0:\n        PrevCum = py_data.loc[(py_data['Insured_Year'] == InsurYr) & (py_data['Year_Only_Lag'] == LagYr - 1), 'cumsum'].values[0]\n    else:\n        PrevCum = Predicted_df.loc[(Predicted_df['InsuredYear'] == InsurYr) & (Predicted_df['PredictedYear_Only_Lag'] == LagYr - 1), 'Predicted_cumsum'].values[0]\n\n    Predicted_df.loc[row, 'Predicted_Incremental'] = CurrCum - PrevCum\n\nPredicted_df[['Predicted_Incremental']] = Predicted_df[['Predicted_Incremental']].astype(float)\nPredictedIncrementalTriangle = pd.pivot_table(Predicted_df, index=[\"InsuredYear\"], columns=[\"PredictedYear_Only_Lag\"],values=[\"Predicted_Incremental\"])\n\n# print(PredictedIncrementalTriangle)\ndisplay(PredictedIncrementalTriangle)","metadata":{"_uuid":"7ce7da4b5e514d476db5b4dd30f4855ef0fff3af","trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"**7.8**\tProject (Future Inflation) Predicted Incremental Amount\n\nNow to project for future inflation. *Unhide to view output\n\nCode Explanation-\n\nThis is rather straightforward as well. First we determine the future inflation index (\"FutureInflation\") via look-up using the year-end-cap plus one as reference.\n\nNow just as before, we simply first equate the future uplifted incremental claims amount derived above to the existing nominal valued as at year-end-cap for easy reference.\n\nLikewise, we also first loop to establish the \"Insured Year\", \"Lag Year\" and \"Current incremental amount\". We then uplift by taking the \"Current incremental amount\" multiplied by the \"FutureInflation\" and the \"Lag Year\" being the index exponent.","metadata":{"_uuid":"60d8b71911ee97a82abf88b5c402939dbee64f69"}},{"cell_type":"code","source":"# Project (Future Inflation) Predicted Incremental Amount\n# Inflated\nFutureInflation = Inflation_df.loc[(Inflation_df['Year'] == (YearEndCap + 1)), 'CumPastInflation'].values[0]\n\nPredicted_df['FutureUplifted_Predicted_Inflated_Incremental'] = Predicted_df['Predicted_Inflated_Incremental']\nfor row in range(0, len(Predicted_df)):\n    InsurYr = Predicted_df.loc[row, 'InsuredYear']\n    LagYr = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    CurrIncremAmt = Predicted_df.loc[row, 'Predicted_Inflated_Incremental']\n    Predicted_df.loc[row, 'FutureUplifted_Predicted_Inflated_Incremental'] = CurrIncremAmt * (FutureInflation ** LagYr)\n    \nprint(Predicted_df['FutureUplifted_Predicted_Inflated_Incremental'])","metadata":{"_uuid":"d0aae957283bff23325deb12b0ae61a5a72a9dce","_kg_hide-output":true,"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# Project (Future Inflation) Predicted Incremental Amount\n# Non-Inflated\n# Set equal for easy reference\nPredicted_df['FutureUplifted_Predicted_Incremental'] = Predicted_df['Predicted_Incremental']\nFutureInflation = Inflation_df.loc[(Inflation_df['Year'] == (YearEndCap + 1)), 'CumPastInflation'].values[0]\n\nfor row in range(0, len(Predicted_df)):\n    InsurYr = Predicted_df.loc[row, 'InsuredYear']\n    LagYr = Predicted_df.loc[row, 'PredictedYear_Only_Lag']\n    CurrIncremAmt = Predicted_df.loc[row, 'Predicted_Incremental']\n\n    Predicted_df.loc[row, 'FutureUplifted_Predicted_Incremental'] = CurrIncremAmt * (FutureInflation ** LagYr)\n    \nprint(Predicted_df['FutureUplifted_Predicted_Incremental'])","metadata":{"_uuid":"02dd72d565f905d546f41a008bb59d6fb04ef576","_kg_hide-output":true,"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"* **8. Preview Predictions (Part C)**\n\nNow lets view what we have done so far!","metadata":{"_uuid":"dfd99fdd5b6649994e49b0288bda3159295039e5"}},{"cell_type":"markdown","source":"**8.1**\tIncremental Amount  ","metadata":{"_uuid":"7f2f5e26ec4408d10acb9a793ef3333cff96948e"}},{"cell_type":"code","source":"# Incremental\n# Non-Inflated\nPredictedTriangle = pd.pivot_table(Predicted_df, index=[\"InsuredYear\"], columns=[\"PredictedYear_Only_Lag\"], values=[\"FutureUplifted_Predicted_Incremental\"])\n# Inflated\nPredictedInflatedTriangle = pd.pivot_table(Predicted_df, index=[\"InsuredYear\"], columns=[\"PredictedYear_Only_Lag\"], values=[\"FutureUplifted_Predicted_Inflated_Incremental\"])","metadata":{"_uuid":"705f125721dc47bf98b5d256b19f49b7dba1200d","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"# Non-Inflated Incremental","metadata":{"_uuid":"6187ffb8055e791ff28fd5caaa8c2548868aa8e7"}},{"cell_type":"code","source":"SinglePlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='FutureUplifted_Predicted_Incremental')\nSubPlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='FutureUplifted_Predicted_Incremental')\ndisplay(PredictedTriangle)","metadata":{"_uuid":"4a70674bcff99e002e073e9118c16036e69b382d","trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"# Inflated Incremental","metadata":{"_uuid":"f8d590a2ff0b7cf2fecfb2a5182bbd2ae03d0698"}},{"cell_type":"code","source":"SinglePlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='FutureUplifted_Predicted_Inflated_Incremental')\nSubPlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='FutureUplifted_Predicted_Inflated_Incremental')\ndisplay(PredictedInflatedTriangle)","metadata":{"_uuid":"df2adf0fe720b9e7941699b42c1b1e8e0ea77cda","trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"**8.2**\tCumulative Amounts","metadata":{"_uuid":"21731d62e05151dfcd75a355afa707667ece05de"}},{"cell_type":"code","source":"# Cumulative\n# Non-Inflated\nPredictedCumTriangle = pd.pivot_table(Predicted_df, index=[\"InsuredYear\"], columns=[\"PredictedYear_Only_Lag\"], values=[\"Predicted_cumsum\"])\n# Inflated\nPredictedInflatedCumTriangle = pd.pivot_table(Predicted_df, index=[\"InsuredYear\"], columns=[\"PredictedYear_Only_Lag\"], values=[\"Predicted_Inflated_cumsum\"])","metadata":{"_uuid":"a1fdb541d2402f3512873077748812f76041d8a8","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"# Non-Inflated Cumulative","metadata":{"_uuid":"47055aa32d9ee5d2bd0424e0b8e1ae85119e9fe1"}},{"cell_type":"code","source":"SinglePlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='Predicted_cumsum')\nSubPlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='Predicted_cumsum')\ndisplay(PredictedCumTriangle)","metadata":{"_uuid":"e218d3abbc5a0f6068b346fad20fd69a243e933e","trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"# Inflated Cumulative","metadata":{"_uuid":"52c7a479c0774ad201c8c3d210e9d553ac91d6cb"}},{"cell_type":"code","source":"SinglePlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='Predicted_Inflated_cumsum')\nSubPlotPartialClaims(DataFrameName=Predicted_df, InsuredYearColumn='InsuredYear', LagYearColumn='PredictedYear_Only_Lag', ValueColumn='Predicted_Inflated_cumsum')\ndisplay(PredictedInflatedCumTriangle)","metadata":{"_uuid":"208c04bb81f0c237f217821c2b438be3974cf6a8","trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"**9.**\t**Full Triangle**","metadata":{"_uuid":"7050419a17005c810d44a051d9128b392b752724"}},{"cell_type":"markdown","source":"**9.0** Define General Plot Functions","metadata":{"_uuid":"107b821866b9a24a8f769e655093cfb95a354cd4"}},{"cell_type":"code","source":"def SinglePlotFullClaims(PastDataFrameName, PastInsuredYearColumn, PastLagYearColumn, PastValueColumn, \n                   FutureDataFrameName, FutureInsuredYearColumn, FutureLagYearColumn, FutureValueColumn):\n    import matplotlib.pyplot as plt\n    from matplotlib import rcParams\n    # https://stackoverflow.com/questions/16419670/increase-distance-between-title-and-plot-in-matplolib\n    \"\"\"Create New df\"\"\"\n    Filtered_NewColumnNames = [\"Insured_Year\",\"Year_Only_Lag\",\"ClaimAmt\"]\n    # Past\n    Past_Filtered_df = pd.DataFrame(PastDataFrameName[[PastInsuredYearColumn, PastLagYearColumn, PastValueColumn]])\n    Past_Filtered_df.columns = Filtered_NewColumnNames\n    # Future\n    Future_Filtered_df = pd.DataFrame(FutureDataFrameName[[FutureInsuredYearColumn, FutureLagYearColumn, FutureValueColumn]])\n    Future_Filtered_df.columns = Filtered_NewColumnNames    \n    \"\"\"Unique Insured Years List\"\"\"\n    # Past\n    Past_InsuredYr_List = list(PastDataFrameName[PastInsuredYearColumn].unique())\n    # Future\n    Future_InsuredYr_List = list(FutureDataFrameName[FutureInsuredYearColumn].unique())\n    \"\"\"Unique Lag Years List\"\"\"\n    # Past\n    Past_LagYr_List = list(PastDataFrameName[PastLagYearColumn].unique())\n    # Future\n    Future_LagYr_List = list(FutureDataFrameName[FutureLagYearColumn].unique())\n    \"\"\"Color List\"\"\"\n    ALL_Colors = ['r','b','g','y','k', 'c', 'm', 'saddlebrown', 'pink', 'lawngreen']         \n    Past_Color_List = ALL_Colors[:len(Past_InsuredYr_List)]\n    Future_Color_List = ALL_Colors[:len(Future_InsuredYr_List)]\n    \"\"\"Plotting\"\"\"\n    fig = plt.figure(2, figsize=(8,12))\n    plt.title('Single Plot Full Claims Data')\n    \"\"\"Full Loop Plot\"\"\"\n    Full_Filtered_df = pd.concat([Past_Filtered_df, Future_Filtered_df])\n    for row_A in range(0,len(Past_InsuredYr_List)):\n        Year_i = Past_InsuredYr_List[row_A]\n        Full_SubFiltered_df = Full_Filtered_df.loc[Full_Filtered_df['Insured_Year'].isin([Year_i])]\n        plt.plot(Full_SubFiltered_df['Year_Only_Lag'], Full_SubFiltered_df['ClaimAmt'], \n                 label=('Predicted %d' % Year_i), linestyle='--', color=Past_Color_List[row_A])\n        plt.legend()\n        plt.xlabel('Developement Year')\n        plt.ylabel('Claims Value')    \n    \"\"\"Past Loop Plot\"\"\"\n    for row_A in range(0,len(Past_InsuredYr_List)):\n        Year_i = Past_InsuredYr_List[row_A]\n        Past_SubFiltered_df = Past_Filtered_df.loc[Past_Filtered_df['Insured_Year'].isin([Year_i])]\n        plt.plot(Past_SubFiltered_df['Year_Only_Lag'], Past_SubFiltered_df['ClaimAmt'], \n                 label=('Historical %d' % Year_i), linestyle='-', color=Past_Color_List[row_A], marker='o')\n        plt.legend()\n    #\"\"\"Future Loop Plot\"\"\"\n    #for row_B in range(0,len(Future_InsuredYr_List)):\n    #    Year_i = Future_InsuredYr_List[row_B]\n    #    Future_SubFiltered_df = Future_Filtered_df.loc[Future_Filtered_df['Insured_Year'].isin([Year_i])]\n    #    plt.plot(Future_SubFiltered_df['Year_Only_Lag'], Future_SubFiltered_df['ClaimAmt'], \n    #             label=str(Year_i), linestyle='--', color=Future_Color_List[row_B])    \n    \n    \"\"\"Plot Attributes\"\"\"    \n    plt.show()","metadata":{"_kg_hide-input":true,"_uuid":"2d510797ffcb0c6e101f1036c281fac1c965f081","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"def SubPlotFullClaims(PastDataFrameName, PastInsuredYearColumn, PastLagYearColumn, PastValueColumn, \n                   FutureDataFrameName, FutureInsuredYearColumn, FutureLagYearColumn, FutureValueColumn):\n    import matplotlib.pyplot as plt\n    from matplotlib import rcParams\n    # https://stackoverflow.com/questions/16419670/increase-distance-between-title-and-plot-in-matplolib\n    \"\"\"Create New df\"\"\"\n    Filtered_NewColumnNames = [\"Insured_Year\",\"Year_Only_Lag\",\"ClaimAmt\"]\n    # Past\n    Past_Filtered_df = pd.DataFrame(PastDataFrameName[[PastInsuredYearColumn, PastLagYearColumn, PastValueColumn]])\n    Past_Filtered_df.columns = Filtered_NewColumnNames\n    # Future\n    Future_Filtered_df = pd.DataFrame(FutureDataFrameName[[FutureInsuredYearColumn, FutureLagYearColumn, FutureValueColumn]])\n    Future_Filtered_df.columns = Filtered_NewColumnNames    \n    \"\"\"Unique Insured Years List\"\"\"\n    # Past\n    Past_InsuredYr_List = list(PastDataFrameName[PastInsuredYearColumn].unique())\n    # Future\n    Future_InsuredYr_List = list(FutureDataFrameName[FutureInsuredYearColumn].unique())\n    \"\"\"Unique Lag Years List\"\"\"\n    # Past\n    Past_LagYr_List = list(PastDataFrameName[PastLagYearColumn].unique())\n    # Future\n    Future_LagYr_List = list(FutureDataFrameName[FutureLagYearColumn].unique())\n    \"\"\"Color List\"\"\"\n    ALL_Colors = ['r','b','g','y','k', 'c', 'm', 'saddlebrown', 'pink', 'lawngreen']         \n    Past_Color_List = ALL_Colors[:len(Past_InsuredYr_List)]\n    Future_Color_List = ALL_Colors[:len(Future_InsuredYr_List)]\n    \"\"\"Plotting\"\"\"\n    fig = plt.figure(2, figsize=(12,16))\n    plt.xticks([]) # remove initial blank plot default ticks\n    plt.yticks([]) # remove initial blank plot default ticks\n    plt.title('Sub Plot Full Claims Data')\n    rcParams['axes.titlepad'] = 50 # position title\n    plt.box(on=None) # Remove boundary line\n    \"\"\"Full Loop Plot\"\"\"\n    Full_Filtered_df = pd.concat([Past_Filtered_df, Future_Filtered_df])\n    i=0\n    for row_A in range(0,len(Past_InsuredYr_List)):\n        ax = fig.add_subplot(5, 2, 1+i)\n        Year_i = Past_InsuredYr_List[row_A]\n        Full_SubFiltered_df = Full_Filtered_df.loc[Full_Filtered_df['Insured_Year'].isin([Year_i])]\n        plt.plot(Full_SubFiltered_df['Year_Only_Lag'], Full_SubFiltered_df['ClaimAmt'], \n                 label=('Predicted %d' % Year_i), linestyle='--', color=Past_Color_List[row_A])\n        plt.legend()\n        i += 1\n        plt.xticks(np.arange(0, (YearEndCap-YearStartCap), step=1))\n        plt.xlabel('Developement Year')\n        plt.ylabel('Claims Value') \n    \"\"\"Past Loop Plot\"\"\"\n    i=0\n    for row_A in range(0,len(Past_InsuredYr_List)):\n        ax = fig.add_subplot(5, 2, 1+i)\n        Year_i = Past_InsuredYr_List[row_A]\n        Past_SubFiltered_df = Past_Filtered_df.loc[Past_Filtered_df['Insured_Year'].isin([Year_i])]\n        plt.plot(Past_SubFiltered_df['Year_Only_Lag'], Past_SubFiltered_df['ClaimAmt'], \n                 label=('Historical %d' % Year_i), linestyle='-', color=Past_Color_List[row_A], marker='o')\n        plt.legend()\n        i += 1\n    #\"\"\"Future Loop Plot\"\"\"\n    #for row_B in range(0,len(Future_InsuredYr_List)):\n    #    Year_i = Future_InsuredYr_List[row_B]\n    #    Future_SubFiltered_df = Future_Filtered_df.loc[Future_Filtered_df['Insured_Year'].isin([Year_i])]\n    #    plt.plot(Future_SubFiltered_df['Year_Only_Lag'], Future_SubFiltered_df['ClaimAmt'], \n    #             label=str(Year_i), linestyle='--', color=Future_Color_List[row_B])    \n    \"\"\"Plot Attributes\"\"\"    \n    fig.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"_uuid":"7166c3cf91ef4bf8b656431347988dfce7042198","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"# 9.1 Non-Inflated Claims","metadata":{"_uuid":"88a0d1ab1a89342e7756451a4663184f17e92f1f"}},{"cell_type":"code","source":"SinglePlotFullClaims(PastDataFrameName=py_data, PastInsuredYearColumn=\"Insured_Year\", PastLagYearColumn=\"Year_Only_Lag\", PastValueColumn=\"cumsum\", \n               FutureDataFrameName=Predicted_df, FutureInsuredYearColumn=\"InsuredYear\", FutureLagYearColumn=\"PredictedYear_Only_Lag\", FutureValueColumn=\"Predicted_cumsum\")","metadata":{"_uuid":"38f6eb395bff42c61b7ef2dd82e0444f50f6276a","trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"SubPlotFullClaims(PastDataFrameName=py_data, PastInsuredYearColumn=\"Insured_Year\", PastLagYearColumn=\"Year_Only_Lag\", PastValueColumn=\"cumsum\", \n               FutureDataFrameName=Predicted_df, FutureInsuredYearColumn=\"InsuredYear\", FutureLagYearColumn=\"PredictedYear_Only_Lag\", FutureValueColumn=\"Predicted_cumsum\")","metadata":{"_uuid":"c3397b0896d643e71cb5f3da997099c5efc4743d","trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"# 9.2 Inflated Claims","metadata":{"_uuid":"8e04ae3dc5f2b6df94fab35bb2fd0b4caaae994a"}},{"cell_type":"code","source":"SinglePlotFullClaims(PastDataFrameName=py_data, PastInsuredYearColumn=\"Insured_Year\", PastLagYearColumn=\"Year_Only_Lag\", PastValueColumn=\"Inflated_cumsum\", \n               FutureDataFrameName=Predicted_df, FutureInsuredYearColumn=\"InsuredYear\", FutureLagYearColumn=\"PredictedYear_Only_Lag\", FutureValueColumn=\"Predicted_Inflated_cumsum\")","metadata":{"_uuid":"4f8994ad9c6631a50870ac8f27ea69895bf9cdf4","trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"SubPlotFullClaims(PastDataFrameName=py_data, PastInsuredYearColumn=\"Insured_Year\", PastLagYearColumn=\"Year_Only_Lag\", PastValueColumn=\"Inflated_cumsum\", \n               FutureDataFrameName=Predicted_df, FutureInsuredYearColumn=\"InsuredYear\", FutureLagYearColumn=\"PredictedYear_Only_Lag\", FutureValueColumn=\"Predicted_Inflated_cumsum\")","metadata":{"_uuid":"5d7a1564a81846b509026c2e796cd78a3b00188c","trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"* **10. Reserves**\n\nLast but simplest step of all. The amount insurers need to cover their predicted claim costs, assuming past trends continue.","metadata":{"_uuid":"191ef1c03b978b127d57b2270fa56673903ce867"}},{"cell_type":"markdown","source":"**10.1** Inflated Amounts","metadata":{"_uuid":"0996fc80fc92832cff24a062c24af7f0dbbcd216"}},{"cell_type":"code","source":"InflatedReserves = Predicted_df['FutureUplifted_Predicted_Inflated_Incremental'].sum()\nprint(InflatedReserves)","metadata":{"_uuid":"b2e7b92d92d8364bd670eb37d9f991443a1fd6e8","trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"**10.2** Non Inflated Amounts","metadata":{"_uuid":"b01c73557cb0e00664459a72000522e574d8fd42"}},{"cell_type":"code","source":"NonInflatedReserves = Predicted_df['FutureUplifted_Predicted_Incremental'].sum()\nprint(NonInflatedReserves)","metadata":{"_uuid":"d75d2b4c599974c5f9d157de1b066516fbcdbccd","trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"PercDiff = 100*(InflatedReserves/NonInflatedReserves-1)\nprint('Percentage Difference {}'.format(PercDiff))","metadata":{"_uuid":"f06bdcf6162c7dad29dcd6ed081713d6016df446","trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\nEvidently, the inflated reserves far exceed that of the non inflated reserves. Despite inflation rates falling, the fact that we uplifted the amounts is a an easy attribution to the reserve results.\n\nDo stay tuned as I plan to do some sensitivity analysis on this going forward!","metadata":{"_uuid":"e26ee5cca0a1fd7369be7648f920614fd8363a26"}},{"cell_type":"markdown","source":"Thank you for reading till the end! Hope you now have a deeper understanding of Data Manipulation using Pandas & also IACL calculations.\n\nCheers!","metadata":{"_uuid":"b28d6e8a82b578a384481502346fa30e5575b2b3"}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"7ed42afaf0e7b2b74ba95030f172a71262b95b39"}}]}
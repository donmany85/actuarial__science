{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He said-\n",
    "\n",
    "I wrote a quick script to backtest one particular method of deriving claims inflation from loss data. I first came across the method in 'Pricing in General Insurance' by Pietro Parodi , but I'm not sure if the method pre-dates the book or not.\n",
    "\n",
    "\n",
    "In order to run the method all we require is a large loss bordereaux, which is useful from a data perspective. Unlike many methods which focus on fitting a curve through attritional loss ratios, or looking at ultimate attritional losses per unit of exposure over time, this method can easily produce a *large loss* inflation pick. Which is important as the two can often be materially different.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code works by simulating 10 years of individual losses from a Poisson-Lognormal model, and then applying 5% inflation pa. \n",
    "We then throw away all losses below the large loss threshold, to put ourselves in the situation as if we'd only been supplied with a large loss claims listing. We then analyse the change over time of the 'median of the top 10 claims'. \n",
    "We select this slightly funky looking statistic as it should increase over time in the presence of inflation, but by looking at the median rather than the mean, we've taken out some of the spikiness. \n",
    "Since we hardcoded 5% inflation into the simulated data, we are looking to arrive back at this value when we apply the method to the  synthetic data.\n",
    "â€‹\n",
    "I've pasted the code below, but jumping to the conclusions, here's a few take-aways:\n",
    "\n",
    "\n",
    "The method does work - note the final answer is 5.04%, and with a few more sims this does appear to approach the original pick of 5%, which is good, the method provides an unbiased estimate.\n",
    "\n",
    "The standard deviation is really high - the standard deviation is of the order of 50% of the mean. Assuming a normal distribution, we'd expect 95% of values to sit between 0%-10% - which is a huge range. In practice even an extra 1% additional inflation in our modelling can often cause a big swing in loss cost, so the method as currently presented, and using this particular set up of simulated loss data is basically useless.\n",
    "\n",
    "The method is thrown off by changes in the FGU claim count  - I haven't shown it below, but if you amend the 'Exposure Growth' value below from 0%, the method no longer provides an unbiased estimate. If the data includes growth, then it tends to over-estimate inflation, and vice-versa if the FGU claim count reduces over time. Parodi does mention this in the book an offers a work-around which I haven't included below, but will write up another time.\n",
    "It's a non-parametric - I do like the fact that it's a non-parametric method. The other large loss inflation methods I'm aware of all involve assuming some underlying probability distribution for the data (exponential, pareto, etc.).\n",
    "\n",
    "We can probably improve the method  - the method effectively ignores all the data other than the 5th largest claim within a given year. So we reduce the entire analysis to the rate of change of just 10 numbers. One obvious extension of the method would be to average across the change in multiple percentiles of the distribution, we could also explore other robust statistics (e.g. Parodi mentions trimmed means?). I'll also set this up another time to see if we get an improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as scipy\n",
    "from math import exp \n",
    "from math import log \n",
    "from math import sqrt \n",
    "from scipy.stats import lognorm \n",
    "from scipy.stats import poisson \n",
    "from scipy.stats import linregress \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distmean=1000000.0\n",
    "DistStdDev=Distmean*1.5\n",
    "AverageFreq=100\n",
    "years=10\n",
    "ExposureGrowth=0.0\n",
    "\n",
    "Mu=log(Distmean/(sqrt(1+DistStdDev**2/Distmean**2)))\n",
    "Sigma=sqrt(log(1+DistStdDev**2/Distmean**2))\n",
    "\n",
    "LLThreshold=1e6 \n",
    "Inflation=0.05 \n",
    "\n",
    "s=Sigma \n",
    "scale=exp(Mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050684669072912254\n",
      "0.026483851446769232\n"
     ]
    }
   ],
   "source": [
    "MedianTop10Method=[]\n",
    "AllLnOutput=[]\n",
    "\n",
    "for sim in range(5000):\n",
    "    SimOutputFGU=[]\n",
    "    SimOutputLL=[]\n",
    "    year=0\n",
    "    Frequency=[]\n",
    "    for year in range(years):\n",
    "        FrequencyInc=poisson.rvs(AverageFreq*(1+ExposureGrowth)**year, size=1)\n",
    "        Frequency.append(FrequencyInc)\n",
    "        r=lognorm.rvs(s,scale=scale,size=FrequencyInc[0])\n",
    "        r=np.multiply(r,(1+Inflation)**year)\n",
    "        r=np.sort(r)[::-1]\n",
    "        r_LLOnly=r[(r>=LLThreshold)]\n",
    "        SimOutputFGU.append(np.transpose(r))\n",
    "        SimOutputLL.append(np.transpose(r_LLOnly))\n",
    "\n",
    "\n",
    "    SimOutputFGU=pd.DataFrame(SimOutputFGU).transpose()\n",
    "    SimOutputLL=pd.DataFrame(SimOutputLL).transpose() \n",
    "    a=np.log(SimOutputLL.iloc[5])\n",
    "    AllLnOutput.append(a)\n",
    "    b=linregress(a.index,a).slope\n",
    "    MedianTop10Method.append(b)\n",
    "\n",
    "AllLnOutput=pd.DataFrame(AllLnOutput)\n",
    "\n",
    "dfMedianTop10Method=pd.DataFrame(MedianTop10Method)\n",
    "dfMedianTop10Method['Exp-1']=np.exp(dfMedianTop10Method[0])-1\n",
    "print(np.mean(dfMedianTop10Method['Exp-1']))\n",
    "print(np.std(dfMedianTop10Method['Exp-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
